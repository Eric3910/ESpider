{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 排列三爬取\n",
    "\n",
    "注： 本程序仅用于爬虫课程学习练手用，原始需求来自同公司老彩民。\n",
    "- 需求：据资深老彩民转述，排列三玩法具有参与容易，数字简单的优势，比较有可能借助于数据分析的方式预测结果。\n",
    "- 后期在机器学习课程内容的基础上，对号码的和值，大小，单双分别统计，并计算每年内最早出现次数，最晚出现次数，平均出现次数等。\n",
    "\n",
    "#### 程序简介：\n",
    "- 首先爬取排列三所有期的日期，期号和开奖号码\n",
    "  - 信息来源： www.lottery.gov.cn/historykj/history.jspx?_ltype=pls\n",
    "- 基于老彩民的意识，爬取北京日出日落时间作为辅助数据.\n",
    "  - 信息来源： https://richurimo.51240.com/beijing__time__2018_12__richurimo/\n",
    "\n",
    "#### 主要思路和体会：\n",
    "##### 排列三抓取\n",
    "- requests for send request and get response\n",
    "- csv for output\n",
    "- BeautifulSoup for parsing html\n",
    "- time for time.sleep to imitate human action.\n",
    "- random for generate random numbers for time.sleep\n",
    "---\n",
    "- 在排列三号码抓取中，使用了传统的requests作为获取html文本的信息，由于request无法执行js脚本，所以在解析文本时，是通过抓取js函数中的参数实现的。\n",
    "- 在实际抓取排列三过程中，由于全部数据为252页。开始时在headers里面只填写了Accept和User_Agent，但发现爬取到20页左右就会被服务器拦截而获取response失败。所以在headers里面加入了更多的内容，使request更像人为完成的。\n",
    "- 在解析网页时使用了beautifulsoup，并且尝试使用了yield关键词\n",
    "---\n",
    "##### 北京日出日落时间抓取\n",
    "- selenium 库用于模拟浏览器，并且Driver.get会自动加载全部信息，js也在其中被执行。\n",
    "- 解析的过程同样使用selenium的find_element_by_xpath\n",
    "- output使用了pandas库的dataframe.to_csv(),额外查找了mode属性，可以变覆盖文档为增加内容。\n",
    "\n",
    "#### 排列三爬取代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "def fetchUrl(url):\n",
    "    '''\n",
    "    Func: fetchUrl is used to get html page content\n",
    "    Parameter:\n",
    "    url : the url for the website \n",
    "    Return : req.test as page content\n",
    "    '''\n",
    "    try:\n",
    "        #imitate browser\n",
    "        headers={'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "                'User_Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:63.0) Gecko/20100101 Firefox/63.0',\n",
    "                'Cookie':'Hm_lvt_8929ffae85e1c07a7ded061329fbf441=1542295767; Hm_lpvt_8929ffae85e1c07a7ded061329fbf441=1542295773; JSESSIONID=CB0636740D359AE52357E4B8266B76E7',\n",
    "                'Connection':'keep-alive',\n",
    "                'Referrer':'www.google.com',\n",
    "                'Accept-Language':'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',\n",
    "                'Cache-Control':'max-age=0'\n",
    "                }\n",
    "        \n",
    "        '''As the server is trying to identify the spider and refuse the request once identified, I found that \n",
    "           with Accept and User_agent is not enough to disguise. So I add more informations like connection, referrer\n",
    "           ,accept_language and Cache-Control. Then the program is much better than before, will be identified after 80 pages\n",
    "           compared with 20 pages without those fields.\n",
    "        '''\n",
    "\n",
    "        #get page content \n",
    "        req = requests.get(url,headers = headers)\n",
    "        # check request status for throw exception\n",
    "        req.raise_for_status() \n",
    "        #encoding the content based on the response from the url\n",
    "        req.encoding=req.apparent_encoding         \n",
    "        #print(req.encoding)\n",
    "        #print('Fetch URL successful.')\n",
    "        return req.text\n",
    "    except Exception as e:\n",
    "        print(e) #throw error\n",
    "\n",
    "\n",
    "def output(dList):\n",
    "    '''\n",
    "    Func : output the result to csv file, name is added with date as a timestamp\n",
    "    Parameters：\n",
    "        dList:the data stored in list\n",
    "    '''\n",
    "    import datetime\n",
    "    today = datetime.date.today().strftime('%y%m%d')\n",
    "    filename = '排列三数据_'+ today + '.csv'   \n",
    "    with open(filename,\"a+\") as csvfile: \n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(dList)\n",
    "    \n",
    "def parsePlsHtml(html):\n",
    "    '''\n",
    "    Func: parseHtml is used to parse HTML Doc.\n",
    "    Parameters：\n",
    "    '''\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml') # beautifulSoup as the parsing tool\n",
    "        i = 0\n",
    "        for item in soup.select('tr')[2:-1]: #neglect the beginning as there are headers \n",
    "            # Since Mr.Li suggest that yield is the benchmark for a python pro.\n",
    "            yield{\n",
    "                'date':item.select('td')[10].text,\n",
    "                'number':item.select('td')[0].text,\n",
    "                'digit1':item.select('td')[1].text.split()[0],\n",
    "                'digit2':item.select('td')[1].text.split()[1],\n",
    "                'digit3':item.select('td')[1].text.split()[2]\n",
    "            }  \n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "\n",
    "def main():\n",
    "    Spider()\n",
    "    \n",
    "def Spider():\n",
    "    '''\n",
    "    Func:调用函数\n",
    "    '''\n",
    "    dList = []\n",
    "    hList=['Date','No','Digit1',\"Digit2\",\"Digit3\"]\n",
    "    dList.append(hList)\n",
    "    for page in range(1,252):\n",
    "        time.sleep(random.random())\n",
    "        print('Page %d'%page)\n",
    "        url = 'http://www.lottery.gov.cn/historykj/history_%d.jspx?_ltype=pls'%page    \n",
    "        htmlPls = fetchUrl(url)\n",
    "        for item in parsePlsHtml(htmlPls):\n",
    "            data = []\n",
    "            data.append(item['date'])\n",
    "            data.append(item['number'])\n",
    "            data.append(item['digit1'])\n",
    "            data.append(item['digit2'])\n",
    "            data.append(item['digit3'])\n",
    "            #dList.append(data)\n",
    "            output(data)  \n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬取北京市日出日落时间代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "def openUrl(driver,url,year,month):\n",
    "    '''\n",
    "    Func : open the url and parse the content via selenium driver.find_elepment_by_xpath\n",
    "    Parameters：\n",
    "        driver: webdriver \n",
    "        dList:the data stored in list\n",
    "        year: year\n",
    "        month: month\n",
    "    '''\n",
    "    driver.get(url)    \n",
    "    #imitate human beings as driver.get() has already gaurantee that the html will be loaded fully.\n",
    "    time.sleep(1)\n",
    "    #find element.\n",
    "    element = driver.find_element_by_xpath(\"//tr\")\n",
    "    dList = []\n",
    "    #split the content by space \n",
    "    dList = element.text.split('\\n')\n",
    "    print('Gathering data for year : %d-%d'%(year,month))\n",
    "    #the first line is the header, ignore it.\n",
    "    return dList[1:-1]\n",
    "            \n",
    "def output(dList,year,month):\n",
    "    '''\n",
    "    Func : output the result to csv file, name is added with date as a timestamp\n",
    "    Parameters：\n",
    "        dList:the data stored in list\n",
    "        year: year\n",
    "        month: month\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    today = datetime.date.today().strftime('%y%m%d')\n",
    "    filename = 'BeijingSunRiseSunDown_'+ today + '.csv'\n",
    "    for data in dList:\n",
    "        # use for loop as the dList contains the data for a month, need to separate, otherwise all \n",
    "        # the data will be written in one line, not the best.\n",
    "        dataframe = pd.DataFrame(data)\n",
    "        dataframe.to_csv(filename,index=False,sep=',',header=False,mode = 'a') \n",
    "        #mode 'a' means add data instead of overriding. \n",
    "    print('data has been added of %d-%d'%(year,month))\n",
    "        \n",
    "def spider():\n",
    "    '''\n",
    "    Func : main function \n",
    "    '''\n",
    "    #A little interaction\n",
    "    startYear = int(input('please input start year'))\n",
    "    print(\"\\n\")\n",
    "    endYear = int(input('please input end year'))\n",
    "    # to make the browser invisible by add headless in options, otherwise it will keep popup and refresh.\n",
    "    chromeOptions = webdriver.ChromeOptions() \n",
    "    chromeOptions.add_argument(\"headless\")\n",
    "    driver = webdriver.Chrome(chrome_options=chromeOptions)\n",
    "    #driver = webdriver.Chrome() # open Chrome webdriver\n",
    "    place = \"beijing\"\n",
    "    data = []\n",
    "    \n",
    "    for year in range(startYear,endYear+1):\n",
    "        for month in range(1,13):\n",
    "            url = \"https://richurimo.51240.com/%s__time__%d_%d__richurimo/\"%(place,year,month)\n",
    "            data.append(openUrl(driver,url,year,month))# get data from html\n",
    "            output(data,year,month) # output data to csv file.\n",
    "        \n",
    "def main():\n",
    "    spider()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
